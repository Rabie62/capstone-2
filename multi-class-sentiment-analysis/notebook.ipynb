{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b009aa5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.utils import resample\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, LSTM, Dense, Dropout, Bidirectional, SpatialDropout1D\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import re\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load Data\n",
    "train_df = pd.read_csv(r\"C:/Desktop/ML bootcamp/multi-class-sentiment-analysis/data/train/train.tsv\", sep='\\t', header=0)\n",
    "test_df = pd.read_csv(r\"C:/Desktop/ML bootcamp/multi-class-sentiment-analysis/data/test/test.tsv\", sep='\\t', header=0)\n",
    "\n",
    "print(f\"Training data: {train_df.shape}\")\n",
    "print(f\"Test data: {test_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e746f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EDA - Sentiment Distribution\n",
    "plt.figure(figsize=(10, 5))\n",
    "sentiment_counts = train_df['Sentiment'].value_counts().sort_index()\n",
    "plt.bar(range(5), sentiment_counts.values, color=['red', 'orange', 'yellow', 'lightgreen', 'green'])\n",
    "plt.title('Sentiment Distribution')\n",
    "plt.xlabel('Sentiment Class')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(range(5), ['Negative', 'Somewhat Negative', 'Neutral', 'Somewhat Positive', 'Positive'])\n",
    "plt.savefig(\"sentiment_distribution.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d53b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We balance the classes so the model learns positive and negative features equally.\n",
    "def balance_data(df):\n",
    "    target_size = 20000 \n",
    "    balanced_df = pd.DataFrame()\n",
    "    \n",
    "    for i in range(5):\n",
    "        class_subset = df[df['Sentiment'] == i]\n",
    "        resampled_class = resample(class_subset,\n",
    "                                   replace=True,        \n",
    "                                   n_samples=target_size, \n",
    "                                   random_state=42)\n",
    "        balanced_df = pd.concat([balanced_df, resampled_class])\n",
    "    \n",
    "    return balanced_df.sample(frac=1).reset_index(drop=True) \n",
    "\n",
    "train_df_balanced = balance_data(train_df)\n",
    "print(\"Balanced Sentiment distribution:\")\n",
    "print(train_df_balanced['Sentiment'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf6a504",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text Preprocessing\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    return ' '.join(text.split())\n",
    "\n",
    "# Initialize preprocessor\n",
    "max_features = 20000\n",
    "max_len = 100\n",
    "tokenizer = Tokenizer(num_words=max_features, oov_token='<OOV>')\n",
    "tokenizer.fit_on_texts(train_df['Phrase'].apply(clean_text))\n",
    "\n",
    "X_train = tokenizer.texts_to_sequences(train_df['Phrase'].apply(clean_text))\n",
    "X_train = pad_sequences(X_train, maxlen=max_len, padding='post', truncating='post')\n",
    "y_train = train_df['Sentiment'].values\n",
    "\n",
    "# Train-Validation Split\n",
    "X_train_split, X_val_split, y_train_split, y_val_split = train_test_split(\n",
    "    X_train, y_train, test_size=0.2, random_state=42, stratify=y_train\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0141136",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Architecture\n",
    "def create_model(vocab_size, max_len=100):\n",
    "    inputs = Input(shape=(max_len,))\n",
    "    embedding = Embedding(vocab_size, 100, input_length=max_len)(inputs)\n",
    "    bilstm = Bidirectional(LSTM(128, return_sequences=True, dropout=0.3))(embedding)\n",
    "    pooled = tf.keras.layers.GlobalMaxPooling1D()(bilstm)\n",
    "    dense1 = Dense(128, activation='relu')(pooled)\n",
    "    dropout = Dropout(0.5)(dense1)\n",
    "    outputs = Dense(5, activation='softmax')(dropout)\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d92b16c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Multiple Models\n",
    "models = {}\n",
    "histories = {}\n",
    "callbacks = [\n",
    "    EarlyStopping(patience=5, restore_best_weights=True),\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=2)\n",
    "]\n",
    "\n",
    "# BiLSTM\n",
    "model1 = create_model(len(tokenizer.word_index) + 1)\n",
    "history1 = model1.fit(X_train_split, y_train_split, validation_data=(X_val_split, y_val_split),\n",
    "                     epochs=30, batch_size=64, callbacks=callbacks, verbose=1)\n",
    "models['BiLSTM'] = model1\n",
    "histories['BiLSTM'] = history1\n",
    "\n",
    "# CNN-LSTM\n",
    "def create_cnn_lstm(vocab_size, max_len=100):\n",
    "    inputs = Input(shape=(max_len,))\n",
    "    embedding = Embedding(vocab_size, 100, input_length=max_len)(inputs)\n",
    "    conv = tf.keras.layers.Conv1D(128, 5, activation='relu')(embedding)\n",
    "    pool = tf.keras.layers.MaxPooling1D(5)(conv)\n",
    "    lstm = LSTM(64)(pool)\n",
    "    outputs = Dense(5, activation='softmax')(lstm)\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "model2 = create_cnn_lstm(len(tokenizer.word_index) + 1)\n",
    "history2 = model2.fit(X_train_split, y_train_split, validation_data=(X_val_split, y_val_split),\n",
    "                     epochs=30, batch_size=64, callbacks=callbacks, verbose=1)\n",
    "models['CNN_LSTM'] = model2\n",
    "histories['CNN_LSTM'] = history2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3cb8b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Evaluation\n",
    "results = {}\n",
    "for name, model in models.items():\n",
    "    print(f\"\\n{'='*20} Evaluating Model: {name} {'='*20}\")\n",
    "    \n",
    "    y_pred_probs = model.predict(X_val_split)\n",
    "    y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "    \n",
    "    accuracy = accuracy_score(y_val_split, y_pred)\n",
    "    results[name] = accuracy\n",
    "    print(f\"Overall Accuracy: {accuracy:.4f}\")\n",
    "    \n",
    "    print(\"\\nDetailed Sentiment Analysis Report:\")\n",
    "    target_names = ['Negative', 'Somewhat Neg', 'Neutral', 'Somewhat Pos', 'Positive']\n",
    "    print(classification_report(y_val_split, y_pred, target_names=target_names))\n",
    "    \n",
    "    # Confusion Matrix Visualization\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    cm = confusion_matrix(y_val_split, y_pred)\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=target_names, yticklabels=target_names)\n",
    "    plt.title(f'Confusion Matrix - {name}')\n",
    "    plt.ylabel('Actual Sentiment')\n",
    "    plt.xlabel('Predicted Sentiment')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d876a536",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Best Model\n",
    "best_model = max(results.keys(), key=lambda x: results[x])\n",
    "models[best_model].save('models/best_model.h5')\n",
    "import pickle\n",
    "with open('models/tokenizer.pkl', 'wb') as f:\n",
    "    pickle.dump(tokenizer, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
