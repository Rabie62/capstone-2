{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b009aa5b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, LSTM, Dense, Dropout, Bidirectional\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Load Data\n",
    "train_df = pd.read_csv('data/train.tsv', sep='\\t', header=0)\n",
    "test_df = pd.read_csv('data/test.tsv', sep='\\t', header=0)\n",
    "\n",
    "print(f\"Training data: {train_df.shape}\")\n",
    "print(f\"Test data: {test_df.shape}\")\n",
    "print(\"Sentiment distribution:\")\n",
    "print(train_df['Sentiment'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e746f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EDA - Sentiment Distribution\n",
    "plt.figure(figsize=(10, 5))\n",
    "sentiment_counts = train_df['Sentiment'].value_counts().sort_index()\n",
    "plt.bar(range(5), sentiment_counts.values, color=['red', 'orange', 'yellow', 'lightgreen', 'green'])\n",
    "plt.title('Sentiment Distribution')\n",
    "plt.xlabel('Sentiment Class')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(range(5), ['Negative', 'Somewhat Negative', 'Neutral', 'Somewhat Positive', 'Positive'])\n",
    "plt.savefig(\"sentiment_distribution.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf6a504",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text Preprocessing\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    return ' '.join(text.split())\n",
    "\n",
    "# Initialize preprocessor\n",
    "max_features = 20000\n",
    "max_len = 100\n",
    "tokenizer = Tokenizer(num_words=max_features, oov_token='<OOV>')\n",
    "tokenizer.fit_on_texts(train_df['Phrase'].apply(clean_text))\n",
    "\n",
    "X_train = tokenizer.texts_to_sequences(train_df['Phrase'].apply(clean_text))\n",
    "X_train = pad_sequences(X_train, maxlen=max_len, padding='post', truncating='post')\n",
    "y_train = train_df['Sentiment'].values\n",
    "\n",
    "# Train-Validation Split\n",
    "X_train_split, X_val_split, y_train_split, y_val_split = train_test_split(\n",
    "    X_train, y_train, test_size=0.2, random_state=42, stratify=y_train\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0141136",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Architecture\n",
    "def create_model(vocab_size, max_len=100):\n",
    "    inputs = Input(shape=(max_len,))\n",
    "    embedding = Embedding(vocab_size, 100, input_length=max_len)(inputs)\n",
    "    bilstm = Bidirectional(LSTM(128, return_sequences=True, dropout=0.3))(embedding)\n",
    "    pooled = tf.keras.layers.GlobalMaxPooling1D()(bilstm)\n",
    "    dense1 = Dense(128, activation='relu')(pooled)\n",
    "    dropout = Dropout(0.5)(dense1)\n",
    "    outputs = Dense(5, activation='softmax')(dropout)\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d92b16c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Multiple Models\n",
    "models = {}\n",
    "histories = {}\n",
    "\n",
    "# BiLSTM\n",
    "model1 = create_model(len(tokenizer.word_index) + 1)\n",
    "history1 = model1.fit(X_train_split, y_train_split, validation_data=(X_val_split, y_val_split),\n",
    "                     epochs=30, batch_size=64, callbacks=[EarlyStopping(patience=5)], verbose=1)\n",
    "models['BiLSTM'] = model1\n",
    "histories['BiLSTM'] = history1\n",
    "\n",
    "# CNN-LSTM\n",
    "def create_cnn_lstm(vocab_size, max_len=100):\n",
    "    inputs = Input(shape=(max_len,))\n",
    "    embedding = Embedding(vocab_size, 100, input_length=max_len)(inputs)\n",
    "    conv = tf.keras.layers.Conv1D(128, 5, activation='relu')(embedding)\n",
    "    pool = tf.keras.layers.MaxPooling1D(5)(conv)\n",
    "    lstm = LSTM(64)(pool)\n",
    "    outputs = Dense(5, activation='softmax')(lstm)\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "model2 = create_cnn_lstm(len(tokenizer.word_index) + 1)\n",
    "history2 = model2.fit(X_train_split, y_train_split, validation_data=(X_val_split, y_val_split),\n",
    "                     epochs=30, batch_size=64, callbacks=[EarlyStopping(patience=5)], verbose=1)\n",
    "models['CNN_LSTM'] = model2\n",
    "histories['CNN_LSTM'] = history2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3cb8b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Evaluation\n",
    "results = {}\n",
    "for name, model in models.items():\n",
    "    y_pred = np.argmax(model.predict(X_val_split), axis=1)\n",
    "    accuracy = accuracy_score(y_val_split, y_pred)\n",
    "    results[name] = accuracy\n",
    "    print(f\"{name}: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d876a536",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Best Model\n",
    "best_model = max(results.keys(), key=lambda x: results[x])\n",
    "models[best_model].save('models/best_model.h5')\n",
    "import pickle\n",
    "with open('models/tokenizer.pkl', 'wb') as f:\n",
    "    pickle.dump(tokenizer, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
